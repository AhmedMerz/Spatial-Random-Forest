{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                                 # to set current working directory \n",
    "import numpy as np                                        # arrays and matrix math\n",
    "import pandas as pd                                       # DataFrames\n",
    "import matplotlib.pyplot as plt                           # plotting\n",
    "cmap = plt.cm.inferno                                     # color map\n",
    "import geostatspy.geostats as geostats\n",
    "import geostatspy.GSLIB as GSLIB\n",
    "import math # For n_effective calculations\n",
    "import scipy # For n_effective calculations\n",
    "import time\n",
    "import json\n",
    "\n",
    "from scipy.integrate import simps\n",
    "from scipy.stats import norm\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intersection_points(x, y):\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        new_x.append(x[i])\n",
    "        new_y.append(y[i])\n",
    "        \n",
    "        if i < len(y) - 1 and np.sign(y[i]) != np.sign(y[i + 1]):\n",
    "            # Sign change detected\n",
    "            x1, x2 = x[i], x[i + 1]\n",
    "            y1, y2 = y[i], y[i + 1]\n",
    "            \n",
    "            # Calculate the x value where the line intersects the x-axis (y = 0)\n",
    "            x_intersection = x1 - (y1 * (x2 - x1)) / (y2 - y1)\n",
    "            \n",
    "            # Add the intersection point to the new arrays\n",
    "            new_x.append(x_intersection)\n",
    "            new_y.append(0)\n",
    "    \n",
    "    return np.array(new_x), np.array(new_y)\n",
    "\n",
    "def goodness(p_intervals, fraction_in, return_plots = True, return_areas = True):\n",
    "    # Example dataset (replace these with your actual data)\n",
    "    x = p_intervals\n",
    "    y = fraction_in - p_intervals\n",
    "\n",
    "    x, y = add_intersection_points(x, y)\n",
    "\n",
    "    # Split the dataset into positive and negative parts of y\n",
    "    positive_mask = y > 0\n",
    "    negative_mask = y < 0\n",
    "\n",
    "    x_positive = x[positive_mask]\n",
    "    y_positive = y[positive_mask]\n",
    "\n",
    "    x_negative = x[negative_mask]\n",
    "    y_negative = y[negative_mask]\n",
    "\n",
    "    # Perform integration on the positive and negative parts separately\n",
    "    try:\n",
    "        area_positive = simps(y_positive, x_positive)\n",
    "    except:\n",
    "        area_positive = 0\n",
    "\n",
    "    try:\n",
    "        area_negative = simps(y_negative, x_negative)\n",
    "    except:\n",
    "        area_negative = 0\n",
    "        \n",
    "    if return_plots:\n",
    "        # Plot the original data and highlight positive and negative parts\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(x, y, label='Original Data', color='blue')\n",
    "        plt.fill_between(x_positive, y_positive, color='green', alpha=0.5, label='Positive Area')\n",
    "        plt.fill_between(x_negative, y_negative, color='red', alpha=0.5, label='Negative Area')\n",
    "        plt.xlabel('p')\n",
    "        plt.ylabel('frac-p')\n",
    "        plt.legend()\n",
    "        plt.title('Accurracy Plot')\n",
    "        plt.grid(True)\n",
    "\n",
    "        #Test 0 points\n",
    "        plt.scatter(x[y==0], y[y==0])\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    \n",
    "    if return_areas:\n",
    "        print(\"Integral of the positive part:\", area_positive)\n",
    "        print(\"Integral of the negative part:\", abs(area_negative))\n",
    "        \n",
    "    G = 1 - (1 * area_positive + 2 * abs(area_negative))\n",
    "    return G\n",
    "\n",
    "def goodness2(p_intervals, fraction_in, return_plots=True, return_areas=True):\n",
    "    # Example dataset (replace these with your actual data)\n",
    "    x = p_intervals\n",
    "    y = fraction_in - p_intervals\n",
    "    \n",
    "    # x, y = add_intersection_points(x, y)\n",
    "\n",
    "    # Calculate the area under the curve (y-values)\n",
    "    try:\n",
    "        area = simps(y, x)\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "    if return_plots:\n",
    "        # Plot the original data\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(x, y, label='Original Data', color='blue')\n",
    "        plt.xlabel('p')\n",
    "        plt.ylabel('frac-p')\n",
    "        plt.legend()\n",
    "        plt.title('Accuracy Plot')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    \n",
    "    if return_areas:\n",
    "        print(\"Integral of the curve:\", area)\n",
    "        \n",
    "    G = 1 - abs(area)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file for vardump not found. Skipping.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import norm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the directory path\n",
    "directory = r\"dataset\"\n",
    "\n",
    "# Get all items in the directory\n",
    "items = os.listdir(directory)\n",
    "\n",
    "# Filter out only directories\n",
    "filename_list = [item for item in items if os.path.isdir(os.path.join(directory, item))]\n",
    "\n",
    "for filenum, filename in enumerate(filename_list):\n",
    "    #### READ DATA AND PARSE\n",
    "    df_path = os.path.join(directory, filename, filename + '.csv')\n",
    "    if not os.path.exists(df_path):\n",
    "        print(f\"Data file for {filename} not found. Skipping.\")\n",
    "        continue\n",
    "    df = pd.read_csv(df_path)\n",
    "    \n",
    "    # We are sampling every 500 meters\n",
    "    df = df.iloc[::5] \n",
    "    df = df.sample(n=500, random_state=73073)\n",
    "    predictors = df.iloc[:, 2:-1].columns.values\n",
    "    response = df.columns[-1]\n",
    "    \n",
    "    df_train, df_test = train_test_split(df, test_size=0.5, random_state=73073)\n",
    "    X_train = df_train.loc[:, predictors].values\n",
    "    y_train = X_train.mean(axis=1)\n",
    "    X_test = df_test.loc[:, predictors].values\n",
    "    y_test = X_test.mean(axis=1)\n",
    "    \n",
    "    # Standardizing the predictor variables\n",
    "    scaler_X = StandardScaler()\n",
    "    X_train = scaler_X.fit_transform(X_train)\n",
    "    X_test = scaler_X.transform(X_test)\n",
    "\n",
    "    # Set constants\n",
    "    max_depth_constant = 3\n",
    "    max_leaf_nodes_constant = 3\n",
    "    n_estimators_constant = 100\n",
    "    num_features = X_train.shape[1]  # Number of features, which should be 9\n",
    "\n",
    "    # Initialize parameters for the spatial bootstrap with bagging\n",
    "    L = n_estimators_constant  # Number of estimators\n",
    "    L_2 = 10  # Number of outer iterations\n",
    "    n_bins = 50  # Number of bins for histograms\n",
    "\n",
    "    print(f\"Processing file {filenum + 1}/{len(filename_list)}: {filename}\")\n",
    "    json_path = os.path.join(directory, filename, filename + '_var_params_and_n_eff.json')\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"JSON file with n_eff for {filename} not found. Skipping.\")\n",
    "        continue\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    #### ASSIGN VARIABLES FROM PARAMS\n",
    "    n_eff_calculated = data['n_eff_calculated']\n",
    "\n",
    "    # Create a list of max_samples values from 10 to 1000 in increments of 10, including n_eff_calculated\n",
    "    max_samples_list = list(range(10, 251, 20))\n",
    "    if n_eff_calculated not in max_samples_list:\n",
    "        max_samples_list.append(n_eff_calculated)\n",
    "        max_samples_list = sorted(max_samples_list)\n",
    "\n",
    "    # Ensure epsilon is defined\n",
    "    epsilon = 1e-10\n",
    "\n",
    "    ### BAGGING APPROACH WITH SENSITIVITY ON max_samples ###\n",
    "    print(f\"Starting Bagging sensitivity analysis on max_samples\")\n",
    "    # Initialize dictionaries to store results for Bagging\n",
    "    bagging_results = {}\n",
    "\n",
    "    # Main Loop for Bagging with varying max_samples\n",
    "    for max_samples_value in max_samples_list:\n",
    "        if max_samples_value > len(X_train):\n",
    "            print(f\"Bagging: max_samples {max_samples_value} exceeds training data size. Skipping.\")\n",
    "            continue\n",
    "        print(f\"Bagging with max_samples = {max_samples_value}\")\n",
    "\n",
    "        # Define base estimator with fixed max_depth and max_leaf_nodes\n",
    "        base_regressor = DecisionTreeRegressor(\n",
    "            max_depth=max_depth_constant,\n",
    "            max_leaf_nodes=max_leaf_nodes_constant,\n",
    "            random_state=73073\n",
    "        )\n",
    "\n",
    "        # Create BaggingRegressor\n",
    "        bagging_regressor = BaggingRegressor(\n",
    "            base_estimator=base_regressor,\n",
    "            n_estimators=L,\n",
    "            max_samples=max_samples_value,\n",
    "            bootstrap=True,\n",
    "            random_state=73073,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        bagging_regressor.fit(X_train, y_train)\n",
    "        estimators = bagging_regressor.estimators_\n",
    "\n",
    "        # Access predictions\n",
    "        reals = np.array([est.predict(X_test) for est in estimators])\n",
    "        trains = np.array([est.predict(X_train) for est in estimators])\n",
    "\n",
    "        # Aggregate predictions\n",
    "        aggregate_test = np.mean(reals, axis=0)\n",
    "        aggregate_train = np.mean(trains, axis=0)\n",
    "\n",
    "        st_dev_test = np.std(reals, axis=0)\n",
    "        st_dev_train = np.std(trains, axis=0)\n",
    "\n",
    "        # Handle zero standard deviation\n",
    "        st_dev_test[st_dev_test == 0] += epsilon\n",
    "        st_dev_train[st_dev_train == 0] += epsilon\n",
    "\n",
    "        # Calculate cdf values for test data\n",
    "        cdf_values_test = norm.cdf(y_test, loc=aggregate_test, scale=st_dev_test)\n",
    "        # Calculate cdf values for train data\n",
    "        cdf_values_train = norm.cdf(y_train, loc=aggregate_train, scale=st_dev_train)\n",
    "\n",
    "        bins = 100\n",
    "        fraction_in_test = np.zeros(bins)\n",
    "        fraction_in_train = np.zeros(bins)\n",
    "        p_intervals = np.linspace(0.0, 1.0, bins)\n",
    "\n",
    "        for i, p in enumerate(p_intervals):\n",
    "            # Test data\n",
    "            test_result_test = (cdf_values_test > 0.5 - 0.5 * p) & (cdf_values_test < 0.5 + 0.5 * p)\n",
    "            fraction_in_test[i] = test_result_test.sum() / len(cdf_values_test)\n",
    "            # Train data\n",
    "            test_result_train = (cdf_values_train > 0.5 - 0.5 * p) & (cdf_values_train < 0.5 + 0.5 * p)\n",
    "            fraction_in_train[i] = test_result_train.sum() / len(cdf_values_train)\n",
    "\n",
    "        # Calculate goodness metrics\n",
    "        goodness_test = goodness(p_intervals, fraction_in_test, return_plots=False, return_areas=False)\n",
    "        goodness_train = goodness(p_intervals, fraction_in_train, return_plots=False, return_areas=False)\n",
    "        goodness2_test = goodness2(p_intervals, fraction_in_test, return_plots=False, return_areas=False)\n",
    "        goodness2_train = goodness2(p_intervals, fraction_in_train, return_plots=False, return_areas=False)\n",
    "\n",
    "        # Calculate MSE\n",
    "        mse_test = mean_squared_error(y_test, aggregate_test)\n",
    "        mse_train = mean_squared_error(y_train, aggregate_train)\n",
    "\n",
    "        # Store results in the dictionary\n",
    "        bagging_results[str(max_samples_value)] = {\n",
    "            'mse_train': mse_train,\n",
    "            'mse_test': mse_test,\n",
    "            'goodness_train': goodness_train,\n",
    "            'goodness_test': goodness_test,\n",
    "            'goodness2_train': goodness2_train,\n",
    "            'goodness2_test': goodness2_test,\n",
    "            'aggregate_train': aggregate_train,\n",
    "            'aggregate_test': aggregate_test,\n",
    "            'fractions_train': fraction_in_train,\n",
    "            'fractions_test': fraction_in_test,\n",
    "            'reals_train': trains,\n",
    "            'reals_test': reals\n",
    "        }\n",
    "\n",
    "    # Create a directory to store the results for this filename\n",
    "    results_dir = os.path.join(directory, filename, 'results_max_samples')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Save the bagging results using NumPy's npz format\n",
    "    np.savez_compressed(\n",
    "        os.path.join(results_dir, filename + '_bagging_results.npz'),\n",
    "        bagging_results=bagging_results\n",
    "    )\n",
    "\n",
    "    ### RANDOM FOREST WITH SENSITIVITY ON max_samples AND max_features ###\n",
    "    print(f\"Starting Random Forest sensitivity analysis on max_samples and max_features\")\n",
    "\n",
    "    # Initialize dictionary to store Random Forest results\n",
    "    rf_results = {}\n",
    "\n",
    "    # Range of max_features to test (from 1 to num_features)\n",
    "    max_features_list = list(range(1, num_features + 1))\n",
    "\n",
    "    # Loop over max_samples values\n",
    "    for max_samples_value in max_samples_list:\n",
    "        if max_samples_value > len(X_train):\n",
    "            print(f\"Random Forest: max_samples {max_samples_value} exceeds training data size. Skipping.\")\n",
    "            continue\n",
    "        print(f\"Random Forest with max_samples = {max_samples_value}\")\n",
    "\n",
    "        # Initialize dictionary for current max_samples_value\n",
    "        rf_results[str(max_samples_value)] = {}\n",
    "\n",
    "        # Loop over max_features values\n",
    "        for max_features_value in max_features_list:\n",
    "            print(f\"  max_features = {max_features_value}\")\n",
    "\n",
    "            # Initialize RandomForestRegressor with varying max_samples and max_features\n",
    "            rf_regressor = RandomForestRegressor(\n",
    "                n_estimators=n_estimators_constant,\n",
    "                max_depth=max_depth_constant,\n",
    "                max_leaf_nodes=max_leaf_nodes_constant,\n",
    "                max_features=max_features_value,  # Varying max_features\n",
    "                max_samples=max_samples_value,\n",
    "                random_state=73073,\n",
    "                n_jobs=-1  # Use all available cores\n",
    "            )\n",
    "\n",
    "            # Fit the model on the training data\n",
    "            rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "            # Predict on test and train data\n",
    "            y_pred_test_rf = rf_regressor.predict(X_test)\n",
    "            y_pred_train_rf = rf_regressor.predict(X_train)\n",
    "\n",
    "            # Calculate MSE\n",
    "            mse_test_rf = mean_squared_error(y_test, y_pred_test_rf)\n",
    "            mse_train_rf = mean_squared_error(y_train, y_pred_train_rf)\n",
    "\n",
    "            # Extract individual tree predictions\n",
    "            reals_rf_test = np.array([tree.predict(X_test) for tree in rf_regressor.estimators_])\n",
    "            reals_rf_train = np.array([tree.predict(X_train) for tree in rf_regressor.estimators_])\n",
    "\n",
    "            # Aggregate predictions\n",
    "            aggregate_rf_test = np.mean(reals_rf_test, axis=0)\n",
    "            aggregate_rf_train = np.mean(reals_rf_train, axis=0)\n",
    "\n",
    "            st_dev_rf_test = np.std(reals_rf_test, axis=0)\n",
    "            st_dev_rf_train = np.std(reals_rf_train, axis=0)\n",
    "\n",
    "            # Handle zero standard deviation\n",
    "            st_dev_rf_test[st_dev_rf_test == 0] += epsilon\n",
    "            st_dev_rf_train[st_dev_rf_train == 0] += epsilon\n",
    "\n",
    "            # Calculate cdf values\n",
    "            cdf_values_rf_test = norm.cdf(y_test, loc=aggregate_rf_test, scale=st_dev_rf_test)\n",
    "            cdf_values_rf_train = norm.cdf(y_train, loc=aggregate_rf_train, scale=st_dev_rf_train)\n",
    "\n",
    "            bins = 100\n",
    "            fraction_in_rf_test = np.zeros(bins)\n",
    "            fraction_in_rf_train = np.zeros(bins)\n",
    "            p_intervals = np.linspace(0.0, 1.0, bins)\n",
    "\n",
    "            for i, p in enumerate(p_intervals):\n",
    "                # Test data\n",
    "                test_result_rf_test = (cdf_values_rf_test > 0.5 - 0.5 * p) & (cdf_values_rf_test < 0.5 + 0.5 * p)\n",
    "                fraction_in_rf_test[i] = test_result_rf_test.sum() / len(cdf_values_rf_test)\n",
    "                # Train data\n",
    "                test_result_rf_train = (cdf_values_rf_train > 0.5 - 0.5 * p) & (cdf_values_rf_train < 0.5 + 0.5 * p)\n",
    "                fraction_in_rf_train[i] = test_result_rf_train.sum() / len(cdf_values_rf_train)\n",
    "\n",
    "            # Calculate goodness metrics\n",
    "            goodness_rf_test = goodness(p_intervals, fraction_in_rf_test, return_plots=False, return_areas=False)\n",
    "            goodness_rf_train = goodness(p_intervals, fraction_in_rf_train, return_plots=False, return_areas=False)\n",
    "            goodness2_rf_test = goodness2(p_intervals, fraction_in_rf_test, return_plots=False, return_areas=False)\n",
    "            goodness2_rf_train = goodness2(p_intervals, fraction_in_rf_train, return_plots=False, return_areas=False)\n",
    "\n",
    "            # Store results under current max_features_value\n",
    "            rf_results[str(max_samples_value)][str(max_features_value)] = {\n",
    "                'mse_train': mse_train_rf,\n",
    "                'mse_test': mse_test_rf,\n",
    "                'goodness_train': goodness_rf_train,\n",
    "                'goodness_test': goodness_rf_test,\n",
    "                'goodness2_train': goodness2_rf_train,\n",
    "                'goodness2_test': goodness2_rf_test,\n",
    "                'aggregate_train': aggregate_rf_train,\n",
    "                'aggregate_test': aggregate_rf_test,\n",
    "                'fractions_train': fraction_in_rf_train,\n",
    "                'fractions_test': fraction_in_rf_test,\n",
    "                'reals_train': reals_rf_train,\n",
    "                'reals_test': reals_rf_test\n",
    "            }\n",
    "\n",
    "    # Save Random Forest results\n",
    "    rf_results_dir = os.path.join(directory, filename, 'rf_results_max_samples_features')\n",
    "    os.makedirs(rf_results_dir, exist_ok=True)\n",
    "\n",
    "    # Save the rf_results using NumPy's npz format\n",
    "    np.savez_compressed(\n",
    "        os.path.join(rf_results_dir, filename + '_rf_results.npz'),\n",
    "        rf_results=rf_results\n",
    "    )\n",
    "\n",
    "    print(f\"Processing for {filename} completed.\")\n",
    "    print(f\"Processed {filenum + 1}/{len(filename_list)} files.\")\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
